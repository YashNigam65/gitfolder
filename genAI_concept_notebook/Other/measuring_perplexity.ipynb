{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3tzv+ZdyXPoFOl8LGjuXz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YashNigam65/gitfolder/blob/master/measuring_perplexity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwI_LZksmBu5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# ------------------------------\n",
        "# Choose a causal language model\n",
        "# ------------------------------\n",
        "MODEL_NAME = \"gpt2\"  # You can change this to any causal LM on HF hub\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "model.eval()  # evaluation mode\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model.to(\"cuda\")\n",
        "\n",
        "\n",
        "def sentence_perplexity(text: str) -> float:\n",
        "    \"\"\"Compute perplexity of a single sentence/string.\"\"\"\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
        "\n",
        "    # For perplexity, we pass labels = input_ids so the model\n",
        "    # tries to predict each next token given previous ones.\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "        loss = outputs.loss  # this is average cross-entropy per token\n",
        "\n",
        "    ppl = math.exp(loss.item())\n",
        "    return ppl\n",
        "\n",
        "\n",
        "def corpus_perplexity(texts):\n",
        "    \"\"\"Compute perplexity over a list of texts (properly weighted by length).\"\"\"\n",
        "    total_log_likelihood = 0.0\n",
        "    total_tokens = 0\n",
        "\n",
        "    for text in texts:\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "            # loss is mean over tokens in this example\n",
        "            n_tokens = inputs[\"input_ids\"].numel()\n",
        "            total_log_likelihood += -outputs.loss.item() * n_tokens\n",
        "            total_tokens += n_tokens\n",
        "\n",
        "    # Average negative log-likelihood per token\n",
        "    avg_neg_log_likelihood = total_log_likelihood / total_tokens\n",
        "    ppl = math.exp(-avg_neg_log_likelihood)\n",
        "    return ppl\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "    print(\"Sentence:\", text)\n",
        "    print(\"Perplexity:\", sentence_perplexity(text))\n",
        "\n",
        "    corpus = [\n",
        "        \"The quick brown fox jumps over the lazy dog.\",\n",
        "        \"Language models estimate the probability of a sequence of words.\",\n",
        "        \"Perplexity is a common metric to evaluate them.\"\n",
        "    ]\n",
        "    print(\"Corpus perplexity:\", corpus_perplexity(corpus))"
      ],
      "metadata": {
        "id": "RNE8JG0Dlz5w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}