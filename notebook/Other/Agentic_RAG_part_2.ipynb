{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2x8BJ4G90DtupiYUZozhu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YashNigam65/gitfolder/blob/master/Agentic_RAG_part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Sequence\n",
        "\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "# -----------------------------\n",
        "# Config\n",
        "# -----------------------------\n",
        "GENERATION_MODEL = \"gemini-2.5-flash\"     # or \"gemini-1.5-flash\" if 2.5 not available\n",
        "EMBEDDING_MODEL = \"text-embedding-004\"    # current text embedding model for RAG\n",
        "\n",
        "client = genai.Client(api_key=\"\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Simple in-memory vector store\n",
        "# -----------------------------\n",
        "\n",
        "@dataclass\n",
        "class DocChunk:\n",
        "    id: int\n",
        "    text: str\n",
        "    embedding: List[float]\n",
        "\n",
        "\n",
        "def cosine_similarity(a: List[float], b: List[float]) -> float:\n",
        "    dot = sum(x * y for x, y in zip(a, b))\n",
        "    na = math.sqrt(sum(x * x for x in a))\n",
        "    nb = math.sqrt(sum(y * y for y in b))\n",
        "    if na == 0 or nb == 0:\n",
        "        return 0.0\n",
        "    return dot / (na * nb)\n",
        "\n",
        "\n",
        "class InMemoryVectorStore:\n",
        "    def __init__(self):\n",
        "        self.chunks: List[DocChunk] = []\n",
        "\n",
        "    def add_texts(self, texts: Sequence[str]) -> None:\n",
        "        \"\"\"Embed and add a list of texts as document chunks.\"\"\"\n",
        "        if not texts:\n",
        "            return\n",
        "\n",
        "        result = client.models.embed_content(\n",
        "            model=EMBEDDING_MODEL,\n",
        "            contents=list(texts),\n",
        "            # Task types recommended for RAG: RETRIEVAL_DOCUMENT / RETRIEVAL_QUERY\n",
        "            config=types.EmbedContentConfig(task_type=\"RETRIEVAL_DOCUMENT\"),\n",
        "        )\n",
        "\n",
        "        start_id = len(self.chunks)\n",
        "        for i, (t, emb) in enumerate(zip(texts, result.embeddings)):\n",
        "            self.chunks.append(\n",
        "                DocChunk(\n",
        "                    id=start_id + i,\n",
        "                    text=t,\n",
        "                    embedding=list(emb.values),\n",
        "                )\n",
        "            )\n",
        "\n",
        "    def similarity_search(self, query: str, k: int = 4) -> List[DocChunk]:\n",
        "        \"\"\"Return top-k most similar chunks to the query.\"\"\"\n",
        "        if not self.chunks:\n",
        "            return []\n",
        "\n",
        "        result = client.models.embed_content(\n",
        "            model=EMBEDDING_MODEL,\n",
        "            contents=[query],\n",
        "            config=types.EmbedContentConfig(task_type=\"RETRIEVAL_QUERY\"),\n",
        "        )\n",
        "        q_emb = list(result.embeddings[0].values)\n",
        "\n",
        "        scored = []\n",
        "        for chunk in self.chunks:\n",
        "            score = cosine_similarity(q_emb, chunk.embedding)\n",
        "            scored.append((score, chunk))\n",
        "\n",
        "        scored.sort(key=lambda x: x[0], reverse=True)\n",
        "        return [c for _, c in scored[:k]]\n",
        "\n",
        "\n",
        "# Single global store for this toy example\n",
        "vector_store = InMemoryVectorStore()\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Tool / function declaration\n",
        "# -----------------------------\n",
        "\n",
        "# Define a function the model can call to retrieve docs\n",
        "search_kb_function = types.FunctionDeclaration(\n",
        "    name=\"search_knowledge_base\",\n",
        "    description=(\n",
        "        \"Search the local document knowledge base and return the most \"\n",
        "        \"relevant text passages for answering the user's question.\"\n",
        "    ),\n",
        "    parameters_json_schema={\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"query\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": (\n",
        "                    \"Natural language query or rephrased question to search with.\"\n",
        "                ),\n",
        "            },\n",
        "            \"k\": {\n",
        "                \"type\": \"integer\",\n",
        "                \"description\": \"Number of passages to retrieve (default 4).\",\n",
        "            },\n",
        "        },\n",
        "        \"required\": [\"query\"],\n",
        "    },\n",
        ")\n",
        "\n",
        "rag_tool = types.Tool(function_declarations=[search_kb_function])\n",
        "\n",
        "BASE_CONFIG = types.GenerateContentConfig(\n",
        "    tools=[rag_tool],\n",
        "    temperature=0.2,  # keep it deterministic for RAG\n",
        ")\n",
        "\n",
        "# New system instruction for the RAG agent\n",
        "SYSTEM_INSTRUCTION = (\n",
        "    \"You are a helpful AI assistant. Your goal is to answer user questions from search_knowledge_base\"\n",
        "    \"accurately and comprehensively. Always strive to integrate information from search_knowledge_base\"\n",
        "    \"to form a complete and coherent answer. If context is provided as part of the search_knowledge_base, limit your reponse \"\n",
        "    \"with it but if it is not available feel free to elaborate or clarify using your foundational knowledge.\"\n",
        "    \"Provide source details along with response: As either from search_knowledge_base content or foundational knowedge\"\n",
        ")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Agentic RAG logic\n",
        "# -----------------------------\n",
        "\n",
        "def agentic_rag(question: str, default_k: int = 4) -> str:\n",
        "    \"\"\"\n",
        "    Ask the Gemini model a question. The model can:\n",
        "      - answer directly, OR\n",
        "      - call the search_knowledge_base function to fetch context, then answer.\n",
        "    \"\"\"\n",
        "    # Embed system instruction directly into the user's question\n",
        "    # This is a workaround as 'system' role is not directly supported in contents list\n",
        "    # for this model/client version.\n",
        "    full_user_prompt = f\"<<SYS>>\\n{SYSTEM_INSTRUCTION}\\n<</SYS>>\\n{question}\"\n",
        "\n",
        "    user_content = types.Content(\n",
        "        role=\"user\",\n",
        "        parts=[types.Part(text=full_user_prompt)],\n",
        "    )\n",
        "\n",
        "    # 1) First call: let Gemini decide whether to call the tool\n",
        "    response = client.models.generate_content(\n",
        "        model=GENERATION_MODEL,\n",
        "        contents=[user_content], # Now only user_content, which includes system instruction\n",
        "        config=BASE_CONFIG,\n",
        "    )\n",
        "\n",
        "    # Quick accessor; returns list of Parts that contain function_call data\n",
        "    function_call_parts = response.function_calls\n",
        "\n",
        "    # If the model decided NOT to call a function, just return its text answer\n",
        "    if not function_call_parts:\n",
        "        return response.text\n",
        "\n",
        "    # For simplicity, handle the first function call only\n",
        "    fn = function_call_parts[0]\n",
        "\n",
        "    if fn.name != \"search_knowledge_base\":\n",
        "        # Unknown tool – just return whatever text we got\n",
        "        return response.text\n",
        "\n",
        "    # Extract args (with fallbacks)\n",
        "    query = fn.args.get(\"query\", question)\n",
        "    k = int(fn.args.get(\"k\", default_k))\n",
        "\n",
        "    # 2) Execute the tool: perform retrieval\n",
        "    docs = vector_store.similarity_search(query, k=k)\n",
        "    function_result = {\n",
        "        \"matches\": [\n",
        "            {\n",
        "                \"rank\": i + 1,\n",
        "                \"text\": d.text,\n",
        "            }\n",
        "            for i, d in enumerate(docs)\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # 3) Build function response part and send back to model\n",
        "    function_response_part = types.Part.from_function_response(\n",
        "        name=\"search_knowledge_base\",\n",
        "        response=function_result,\n",
        "    )\n",
        "    function_response_content = types.Content(\n",
        "        role=\"tool\",\n",
        "        parts=[function_response_part],\n",
        "    )\n",
        "\n",
        "    # Second call: give the model (a) the original user question (now including system instruction)\n",
        "    # (b) its own function-call content, and (c) the tool output\n",
        "    response2 = client.models.generate_content(\n",
        "        model=GENERATION_MODEL,\n",
        "        contents=[\n",
        "            user_content, # This already includes the system instruction\n",
        "            response.candidates[0].content,  # includes the function_call part\n",
        "            function_response_content,\n",
        "        ],\n",
        "        config=BASE_CONFIG,\n",
        "    )\n",
        "\n",
        "    return response2.text\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Demo / usage\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Index some toy docs. In real RAG you’d chunk + load PDFs, DB rows, etc.\n",
        "    corpus = [\n",
        "        \"Retrieval-Augmented Generation (RAG) combines a language model with \",\n",
        "        \"a retriever that fetches relevant documents from a knowledge base.\",\n",
        "\n",
        "        \"Gemini is a family of multimodal models from Google that can accept \",\n",
        "        \"text, images, audio and more in a single prompt.\",\n",
        "\n",
        "        \"In RAG, embeddings are used to represent both user queries and \",\n",
        "        \"documents as vectors so that semantic similarity search can be done.\",\n",
        "\n",
        "        \"Chunking long documents into smaller passages often improves RAG \",\n",
        "        \"retrieval quality and reduces prompt token usage.\",\n",
        "    ]\n",
        "    vector_store.add_texts(corpus)\n",
        "\n",
        "    print(\"Simple Agentic RAG demo with Gemini. Empty question to exit.\\n\")\n",
        "\n",
        "    while True:\n",
        "        q = input(\"You: \").strip()\n",
        "        if not q:\n",
        "            break\n",
        "        ans = agentic_rag(q)\n",
        "        print(\"\\nAssistant:\", ans, \"\\n\" + \"-\" * 60 + \"\\n\")"
      ],
      "metadata": {
        "id": "lH8qWfTQTAwG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}