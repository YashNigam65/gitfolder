{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet uses the gensim.downloader library to download and load a pre-trained word embedding model called 'glove-wiki-gigaword-100'. This model represents words as numerical vectors, capturing their semantic meaning. After loading the model, it demonstrates how to retrieve the embedding (vector) for the word 'example' and prints its characteristics like dimension, data type, and shape."
      ],
      "metadata": {
        "id": "yMPluOMuDrj0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "new_cell_1",
        "outputId": "72e0d439-4e9a-4ef2-8cf9-fd666f820ff6"
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.4.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.0)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " In the context of the word embedding, these terms describe the characteristics of the numerical vector representing the word:\n",
        "\n",
        "**Dimension (100)**: This refers to the length or size of the vector. In this case, the 'glove-wiki-gigaword-100' model creates 100-dimensional vectors, meaning each word is represented by 100 numbers.\n",
        "\n",
        "**Type (<class 'numpy.ndarray'>)**: This indicates that the embedding is a NumPy array. NumPy is a popular Python library for numerical computing, especially with arrays and matrices.\n",
        "\n",
        "**Shape ((100,))**: This describes the structure of the array. (100,) means it's a one-dimensional array (a vector) with 100 elements. If it were a 2D array (like a table), it might look something like (rows, columns)."
      ],
      "metadata": {
        "id": "Bj7Jf8OHE-jE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67Iwbmo4kUUZ",
        "outputId": "2ee9a565-9ce3-4166-dfd1-f5903f0717ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding for 'example':\n",
            "[-0.12617    0.61724    0.22581    0.39868    0.16111    0.1523\n",
            " -0.14715   -0.29447   -0.27348   -0.13753   -0.20898   -0.73436\n",
            "  0.14144    0.15048    0.09179    0.018613   0.22539    0.15979\n",
            " -0.16935    0.42716    0.042284  -0.3477    -0.11413    0.12222\n",
            " -0.025027  -0.20805   -0.067264  -0.2956    -0.30807   -0.32903\n",
            "  0.19059    0.77141   -0.19332   -0.31069    0.26745    0.32231\n",
            "  0.2065     0.10497    0.49425   -0.38322   -0.12802   -0.069906\n",
            " -0.14828    0.085369  -0.18141    0.14688    0.60968   -0.21131\n",
            " -0.29148   -0.52773    0.59508    0.017369   0.15342    0.81925\n",
            " -0.20643   -2.0378    -0.11884   -0.16826    1.5288     0.15756\n",
            " -0.4994     0.39305    0.12672   -0.10968    1.3671    -0.21006\n",
            "  0.15684    0.0063801  0.43836   -0.18765   -0.29088    0.18619\n",
            "  0.085402   0.13985    0.40794   -0.14811    0.26702   -0.19142\n",
            " -0.6189     0.0091217  0.34971   -0.24079   -0.52476   -0.25071\n",
            " -1.5681     0.22101    0.046796  -0.62616   -0.043358  -0.42865\n",
            " -0.0057843 -0.22611    0.074171   0.091597  -0.40751   -0.08359\n",
            " -0.48413   -1.0718     0.52827    0.058813 ]\n",
            "Dimension: 100\n",
            "Type: <class 'numpy.ndarray'>\n",
            "Shape: (100,)\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Download and load the pre-trained GloVe model\n",
        "model = api.load(\"glove-wiki-gigaword-100\")  # You can choose different dimensions like 50, 100, 200, 300\n",
        "\n",
        "# Example usage: get the embedding for a word\n",
        "word = 'example'\n",
        "embedding = model[word]\n",
        "\n",
        "print(f\"Embedding for '{word}':\\n{embedding}\")\n",
        "print(\"Dimension:\", len(embedding))\n",
        "print(\"Type:\", type(embedding))\n",
        "print(\"Shape:\", embedding.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It calculates the analogy 'king' - 'man' + 'woman' to find words that are semantically similar to the result. It then prints the top 5 most similar words along with their similarity scores."
      ],
      "metadata": {
        "id": "Dj8MVFtyF42m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "model.most_similar(...): This is a method of your loaded word embedding model (the GloVe model in this case). Its purpose is to find words in the vocabulary that are most semantically similar to a target concept, which is derived from the positive and negative lists you provide.\n",
        "\n",
        "positive=['king', 'woman']: This argument takes a list of words whose vectors you want to add together. In an analogy, these words contribute to the desired semantic direction. Here, you're starting with the concept of 'king' and adding the concept of 'woman'.\n",
        "\n",
        "negative=['man']: This argument takes a list of words whose vectors you want to subtract. In the analogy 'king' - 'man' + 'woman', you are subtracting the 'man-ness' from 'king'. The combined effect is to transform 'king' along a 'gender' dimension towards 'woman'.\n",
        "\n",
        "topn=5: This argument specifies that you want the function to return the top 5 words that are most similar to the final vector generated by the positive and negative additions/subtractions. These are the words with the highest cosine similarity to the resultant vector."
      ],
      "metadata": {
        "id": "ShOdVzunHp0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we do vector('king') - vector('man'):\n",
        "\n",
        "vector('king'): This represents the concept of a male monarch.\n",
        "- vector('man'): By subtracting the 'man' vector, we are essentially trying to remove the 'maleness' and specific attributes associated with 'man' from 'king'. What's left (conceptually) is the 'royalty' or 'leadership' aspect, minus the gender.\n",
        "Then, when we add + vector('woman'), we are trying to add the 'femaleness' and attributes associated with 'woman' to that remaining 'royalty/leadership' concept. The goal is to find a word that embodies 'royalty/leadership' plus 'woman-ness'.\n",
        "\n",
        "So, the subtraction of 'man' is crucial for isolating the semantic component (like gender) that we then want to transfer or modify with another word's component."
      ],
      "metadata": {
        "id": "5sxuPSLsHzBk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5b60eb5",
        "outputId": "2c4739c9-2ba1-4001-e202-f94b6f7173ae"
      },
      "source": [
        "result = model.most_similar(positive=['king', 'woman'], negative=['man'], topn=5)\n",
        "print(\"Analogy 'king' - 'man' + 'woman':\")\n",
        "for word, similarity in result:\n",
        "    print(f\"  {word}: {similarity:.4f}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analogy 'king' - 'man' + 'woman':\n",
            "  queen: 0.7699\n",
            "  monarch: 0.6843\n",
            "  throne: 0.6756\n",
            "  daughter: 0.6595\n",
            "  princess: 0.6521\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# finding most simlar word\n",
        "result = model.most_similar(positive=['woman'],  topn=5)\n",
        "\n",
        "for word, similarity in result:\n",
        "    print(f\"  {word}: {similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WNjo3XEHLHk",
        "outputId": "f7fbae7c-ad6e-4888-adb3-7affcf701be9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  girl: 0.8473\n",
            "  man: 0.8323\n",
            "  mother: 0.8276\n",
            "  boy: 0.7721\n",
            "  she: 0.7632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "similar_words = model.most_similar(positive=['king'], topn=10)\n",
        "print(\"Words similar to 'king':\")\n",
        "for word, similarity in similar_words:\n",
        "    print(f\"  {word}: {similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gspw0y6XG-oa",
        "outputId": "4fcabe1a-72a4-4f39-b340-e3f72cf2c395"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words similar to 'king':\n",
            "  prince: 0.7682\n",
            "  queen: 0.7508\n",
            "  son: 0.7021\n",
            "  brother: 0.6986\n",
            "  monarch: 0.6978\n",
            "  throne: 0.6920\n",
            "  kingdom: 0.6811\n",
            "  father: 0.6802\n",
            "  emperor: 0.6713\n",
            "  ii: 0.6676\n"
          ]
        }
      ]
    }
  ]
}