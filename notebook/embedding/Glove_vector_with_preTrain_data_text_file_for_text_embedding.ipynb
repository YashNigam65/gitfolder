{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "To run this notebook you need to first download \"glove.6B.50d\" file from goggle and put in sample folder and give path as '/content/sample_data/glove.6B.50d.txt', you can found this file in our laptop local storage as well."
      ],
      "metadata": {
        "id": "ZwiBflKiJ8SO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is set up to demonstrate how to use pre-trained GloVe word embeddings for a given vocabulary.\n"
      ],
      "metadata": {
        "id": "Vd_fieWBKXit"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell imports necessary libraries: Tokenizer and pad_sequences (addresses this by adding zeros to the beginning or end of sequences (known as 'padding') to make them all the same length. If a sequence is longer than the desired fixed length, it can also be truncated.) from tensorflow.keras.preprocessing.text for text preprocessing, and numpy for numerical operations. These are fundamental for tasks involving natural language processing (NLP)."
      ],
      "metadata": {
        "id": "9UBGhYTLKgWM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5omHLLytMX3u"
      },
      "outputs": [],
      "source": [
        "# code for Glove word embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines a Python function called embedding_for_vocab. This function is crucial for creating an embedding matrix. It reads a GloVe embedding file (a file containing words and their corresponding dense vector representations), and for each word in your provided word_index (which maps words to unique integers), it populates an embedding_matrix_vocab with the pre-trained GloVe vector for that word. If a word isn't found in the GloVe file, its vector will remain zeros."
      ],
      "metadata": {
        "id": "W6ZcGmDfLCOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "vocab_size = len(word_index) + 1: This line calculates the total number of unique words in your vocabulary. It takes the length of word_index (which maps each unique word to an integer) and adds 1. The + 1 is important because word_index typically starts numbering words from 1, reserving 0 for padding or unknown words in many NLP applications."
      ],
      "metadata": {
        "id": "Z8GdOCHGYMdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embedding_for_vocab(filepath, word_index,\n",
        "\t\t\t\t\t\tembedding_dim):\n",
        "\tvocab_size = len(word_index) + 1\n",
        "\tprint('vocab_size', vocab_size)\n",
        "\t# Adding again 1 because of reserved 0 index\n",
        "\tembedding_matrix_vocab = np.zeros((\n",
        "\t\t\tvocab_size, embedding_dim))\n",
        "\tprint('embedding_matrix_vocab',embedding_matrix_vocab)\n",
        "\n",
        "\twith open(filepath, encoding=\"utf8\") as f:\n",
        "\t\tfor line in f:\n",
        "\t\t\tword, *vector = line.split()\n",
        "\t\t\tif word in word_index:\n",
        "\t\t\t\tidx = word_index[word]\n",
        "\t\t\t\tembedding_matrix_vocab[idx] = np.array(\n",
        "\t\t\t\t\tvector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "\treturn embedding_matrix_vocab"
      ],
      "metadata": {
        "id": "-kJXrQmwN1wO"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "line.split(): This part of the code takes the line (which is a string, usually read from the GloVe file) and splits it into a list of substrings. By default, split() divides the string by any whitespace (spaces, tabs, newlines) and discards empty strings, giving you a list of words and numbers.\n",
        "\n",
        "word, *vector = ...: This is Python's tuple unpacking (or sequence unpacking) with an asterisk (*) operator. It works like this:\n",
        "\n",
        "The first element returned by line.split() is assigned to the variable word.\n",
        "All the remaining elements from the line.split() list are collected into a new list, which is then assigned to the variable vector."
      ],
      "metadata": {
        "id": "aaI-sWCAZ6-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "tokenizer.fit_on_texts(x): This is the \"training\" step for the tokenizer. You pass it your text data (in this case, the set x which contains words like 'text', 'the', 'leader', etc.). The fit_on_texts method then:\n",
        "\n",
        "Scans through all the words in x.\n",
        "Identifies all unique words.\n",
        "Assigns a unique integer index to each unique word. This mapping is stored internally within the tokenizer object, specifically in its word_index attribute (which you saw printed in the previous output as a dictionary). The indices typically start from 1, reserving 0 for padding or unknown words."
      ],
      "metadata": {
        "id": "r4wxaxDkmktF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# x = {'text', 'the', 'the', 'prime',\n",
        "# \t'natural', 'language'}\n",
        "\n",
        "# # create the dict.\n",
        "# tokenizer = Tokenizer()\n",
        "# tokenizer.fit_on_texts(x)\n",
        "\n",
        "# # number of unique words in dict.\n",
        "# print(\"Number of unique words in dictionary=\",\n",
        "# \tlen(tokenizer.word_index))\n",
        "# print(\"Dictionary is = \", tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FAY0p7AmWVG",
        "outputId": "ac687698-4a74-41e1-c4cc-a230d3f871a5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique words in dictionary= 5\n",
            "Dictionary is =  {'the': 1, 'text': 2, 'language': 3, 'natural': 4, 'prime': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = {'text', 'the', 'leader', 'prime',\n",
        "\t'natural', 'language'}\n",
        "\n",
        "# create the dict.\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(x)\n",
        "\n",
        "# number of unique words in dict.\n",
        "print(\"Number of unique words in dictionary=\",\n",
        "\tlen(tokenizer.word_index))\n",
        "print(\"Dictionary is = \", tokenizer.word_index)\n",
        "\n",
        "\n",
        "\n",
        "# matrix for vocab: word_index\n",
        "embedding_dim = 50\n",
        "embedding_matrix_vocab = embedding_for_vocab(\n",
        "\t'/content/sample_data/glove.6B.50d.txt', tokenizer.word_index,\n",
        "embedding_dim)\n",
        "\n",
        "print(\"Dense vector is => \",\n",
        "\tembedding_matrix_vocab)\n",
        "\n",
        "print(\"Dense vector for first word is => \",\n",
        "\tembedding_matrix_vocab[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-EXNV2lMctR",
        "outputId": "d3a9a46a-9234-4c4a-c514-a9016979e168"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique words in dictionary= 6\n",
            "Dictionary is =  {'the': 1, 'text': 2, 'language': 3, 'natural': 4, 'prime': 5, 'leader': 6}\n",
            "vocab_size 7\n",
            "embedding_matrix_vocab [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0.]]\n",
            "Dense vector is =>  [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00]\n",
            " [ 4.18000013e-01  2.49679998e-01 -4.12420005e-01  1.21699996e-01\n",
            "   3.45270008e-01 -4.44569997e-02 -4.96879995e-01 -1.78619996e-01\n",
            "  -6.60229998e-04 -6.56599998e-01  2.78430015e-01 -1.47670001e-01\n",
            "  -5.56770027e-01  1.46579996e-01 -9.50950012e-03  1.16579998e-02\n",
            "   1.02040000e-01 -1.27920002e-01 -8.44299972e-01 -1.21809997e-01\n",
            "  -1.68009996e-02 -3.32789987e-01 -1.55200005e-01 -2.31309995e-01\n",
            "  -1.91809997e-01 -1.88230002e+00 -7.67459989e-01  9.90509987e-02\n",
            "  -4.21249986e-01 -1.95260003e-01  4.00710011e+00 -1.85939997e-01\n",
            "  -5.22870004e-01 -3.16810012e-01  5.92130003e-04  7.44489999e-03\n",
            "   1.77780002e-01 -1.58969998e-01  1.20409997e-02 -5.42230010e-02\n",
            "  -2.98709989e-01 -1.57490000e-01 -3.47579986e-01 -4.56370004e-02\n",
            "  -4.42510009e-01  1.87849998e-01  2.78489990e-03 -1.84110001e-01\n",
            "  -1.15139998e-01 -7.85809994e-01]\n",
            " [ 3.26150000e-01  3.66860002e-01 -7.49049988e-03 -3.75530005e-01\n",
            "   6.67150021e-01  2.16460004e-01 -1.98009998e-01 -1.10010004e+00\n",
            "  -4.22210008e-01  1.05740003e-01 -3.12920004e-01  5.09530008e-01\n",
            "   5.57749987e-01  1.20190002e-01  3.14410001e-01 -2.50429988e-01\n",
            "  -1.06369996e+00 -1.32130003e+00  8.77979994e-01 -2.46270001e-01\n",
            "   2.73790002e-01 -5.10919988e-01  4.93239999e-01  5.22430003e-01\n",
            "   1.16359997e+00 -7.53229976e-01 -4.80529994e-01 -1.12590000e-01\n",
            "  -5.45949996e-01 -8.39209974e-01  2.98250008e+00 -1.19159997e+00\n",
            "  -5.19580007e-01 -3.93649995e-01 -1.41900003e-01 -2.69770008e-02\n",
            "   6.62959993e-01  1.65739998e-01 -1.16810000e+00  1.44429997e-01\n",
            "   1.63049996e+00 -1.72160000e-01 -1.74360007e-01 -1.04900002e-02\n",
            "  -1.77939996e-01  9.30760026e-01  1.03810000e+00  9.42659974e-01\n",
            "  -1.48049995e-01 -6.11090004e-01]\n",
            " [-5.79900026e-01 -1.10100001e-01 -1.15569997e+00 -2.99059995e-03\n",
            "  -2.06129998e-01  4.52890009e-01 -1.66710004e-01 -1.03820002e+00\n",
            "  -9.92410004e-01  3.98840010e-01  5.92299998e-01  2.29900002e-01\n",
            "   1.52129996e+00 -1.77640006e-01 -2.97259986e-01 -3.92349988e-01\n",
            "  -7.84709990e-01  1.55939996e-01  6.90769970e-01  5.95369995e-01\n",
            "  -4.43399996e-01  5.35139978e-01  3.28530014e-01  1.24370003e+00\n",
            "   1.29719996e+00 -1.38779998e+00 -1.09249997e+00 -4.09249991e-01\n",
            "  -5.69710016e-01 -3.46560001e-01  3.71630001e+00 -1.04890001e+00\n",
            "  -4.67079997e-01 -4.47389990e-01  6.22999994e-03  1.96490008e-02\n",
            "  -4.01609987e-01 -6.29130006e-01 -8.25060010e-01  4.55909997e-01\n",
            "   8.26259971e-01  5.70909977e-01  2.11989999e-01  4.68650013e-01\n",
            "  -6.00269973e-01  2.99199998e-01  6.79440022e-01  1.42379999e+00\n",
            "  -3.21520008e-02 -1.26029998e-01]\n",
            " [ 4.42649990e-01  8.47649992e-01 -4.59800005e-01  6.79929972e-01\n",
            "   1.38410002e-01  3.94560009e-01 -1.73429996e-01 -6.40550017e-01\n",
            "   8.64390016e-01  8.16240013e-01  7.57380009e-01  4.11430001e-01\n",
            "   1.09350002e+00 -3.00680012e-01 -8.48599970e-02  5.17840028e-01\n",
            "   1.08700001e+00  4.50610012e-01 -4.95950013e-01 -6.06500030e-01\n",
            "  -1.67490005e-01 -2.85569996e-01 -4.37190011e-02 -8.61540020e-01\n",
            "   3.39599997e-01 -7.52399981e-01 -3.32060009e-01  2.46680006e-01\n",
            "   1.00209999e+00  7.19229996e-01  3.30769992e+00 -6.47549987e-01\n",
            "   1.65099993e-01 -9.16509986e-01 -3.53629999e-02  2.17940003e-01\n",
            "  -8.78970027e-01  3.78010005e-01  6.67330027e-01  4.20540005e-01\n",
            "  -2.13870004e-01  1.59170002e-01  5.22449970e-01  2.05870003e-01\n",
            "  -1.67140007e-01  5.80579996e-01 -3.68279994e-01  3.55710015e-02\n",
            "   1.40990000e-02 -2.48170003e-01]\n",
            " [ 5.07950008e-01  6.98819995e-01  4.14680004e-01  4.99729991e-01\n",
            "   8.27310026e-01  5.88829994e-01 -4.34080005e-01  2.17030004e-01\n",
            "  -1.81809998e+00 -7.42739975e-01 -1.79910004e-01  2.84929991e-01\n",
            "  -1.69369996e-01  8.74490023e-01  5.52940011e-01  9.10309970e-01\n",
            "   2.19569996e-01 -4.85100001e-01  7.54890025e-01  5.23419976e-01\n",
            "   5.43799996e-01  1.01080000e-01 -7.91900009e-02 -1.14780001e-01\n",
            "   2.94739991e-01 -1.60039997e+00  5.28540015e-01  4.08399999e-02\n",
            "  -7.19799995e-01  1.93540001e+00  2.81900001e+00  6.07159972e-01\n",
            "  -1.12080002e+00  5.71939982e-02  1.43099993e-01  4.73720014e-01\n",
            "   5.95809996e-01  1.13810003e-01 -7.99549997e-01 -2.80869991e-01\n",
            "  -3.28969985e-01  1.32560003e+00 -1.18040001e+00 -1.38600004e+00\n",
            "   2.02020004e-01  5.14869988e-01 -1.90680003e+00  6.54190004e-01\n",
            "   1.72459996e+00 -6.01300001e-01]\n",
            " [-1.56700000e-01  2.61170000e-01  7.88810015e-01  6.52069986e-01\n",
            "   1.20019996e+00  3.54009986e-01 -3.42979997e-01  3.17019999e-01\n",
            "  -1.15020001e+00 -1.60990000e-01  1.57979995e-01 -5.35019994e-01\n",
            "  -1.34679997e+00  5.17830014e-01 -4.64410007e-01 -1.98459998e-01\n",
            "   2.74749994e-01 -2.61539996e-01  2.55309999e-01  3.33880007e-01\n",
            "  -1.04130006e+00  5.25250018e-01 -3.54429990e-01 -1.91369995e-01\n",
            "  -8.96399990e-02 -2.33139992e+00  1.24329999e-01 -9.44050014e-01\n",
            "  -1.02330005e+00  1.35070002e+00  2.55240011e+00 -1.68970004e-01\n",
            "  -1.72899997e+00  3.25480014e-01 -3.09139997e-01 -6.30569994e-01\n",
            "  -2.22110003e-01 -1.55890003e-01 -4.35979992e-01  5.68000004e-02\n",
            "  -9.08849984e-02  7.50280023e-01 -1.31529999e+00 -7.53589988e-01\n",
            "   8.28989983e-01  5.13969995e-02 -1.48049998e+00 -1.11340001e-01\n",
            "   2.70900011e-01 -4.87129986e-01]]\n",
            "Dense vector for first word is =>  [ 4.18000013e-01  2.49679998e-01 -4.12420005e-01  1.21699996e-01\n",
            "  3.45270008e-01 -4.44569997e-02 -4.96879995e-01 -1.78619996e-01\n",
            " -6.60229998e-04 -6.56599998e-01  2.78430015e-01 -1.47670001e-01\n",
            " -5.56770027e-01  1.46579996e-01 -9.50950012e-03  1.16579998e-02\n",
            "  1.02040000e-01 -1.27920002e-01 -8.44299972e-01 -1.21809997e-01\n",
            " -1.68009996e-02 -3.32789987e-01 -1.55200005e-01 -2.31309995e-01\n",
            " -1.91809997e-01 -1.88230002e+00 -7.67459989e-01  9.90509987e-02\n",
            " -4.21249986e-01 -1.95260003e-01  4.00710011e+00 -1.85939997e-01\n",
            " -5.22870004e-01 -3.16810012e-01  5.92130003e-04  7.44489999e-03\n",
            "  1.77780002e-01 -1.58969998e-01  1.20409997e-02 -5.42230010e-02\n",
            " -2.98709989e-01 -1.57490000e-01 -3.47579986e-01 -4.56370004e-02\n",
            " -4.42510009e-01  1.87849998e-01  2.78489990e-03 -1.84110001e-01\n",
            " -1.15139998e-01 -7.85809994e-01]\n"
          ]
        }
      ]
    }
  ]
}